{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215b12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from scipy.stats import chi2\n",
    "import math\n",
    "import arch\n",
    "import time\n",
    "import pygosolnp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def zero_mean_test(data, true_mu=0, conf_level=0.95):\n",
    "    ''' Perfom a t-Test if mean of distribution:\n",
    "         - null hypothesis (H0) = zero\n",
    "         - alternative hypothesis (H1) != zero\n",
    "         \n",
    "        Parameters:\n",
    "            data (dataframe):   pnl (distribution of profit and loss) or return\n",
    "            true_mu (float):    expected mean of distribuition\n",
    "            conf_level (float): test confidence level\n",
    "        Returns:\n",
    "            answer (dict):      statistics and decision of the test\n",
    "    '''\n",
    "\n",
    "    significance = 1-conf_level\n",
    "    \n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data, ddof=1)\n",
    "    \n",
    "    t = (mean - true_mu)/(std/np.sqrt(len(data)))\n",
    "    '''p<0.05, 2-tail'''\n",
    "    t_padrao = stats.t.ppf(1-round(significance/2,4), len(data)-1)\n",
    "    pvalue = stats.ttest_1samp(data, popmean=true_mu, alternative='two-sided')[-1]\n",
    "    H0 = \"Mean of distribution = 0\"\n",
    "    if pvalue > significance: #ou t < np.abs(t_padrao): \n",
    "        decision = \"Fail to rejected H0.\"\n",
    "    else:\n",
    "        decision = \"Reject H0.\"\n",
    "\n",
    "    answer = {\"null hypothesis\":H0,\n",
    "              \"decision\":decision,\n",
    "              \"t-test statistc\":t,\n",
    "              \"t-tabuladed\":t_padrao,\n",
    "              \"p-value\":pvalue}\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def duration_test(violations, conf_level=0.95):\n",
    "    \n",
    "    '''Perform the Christoffersen and Pelletier Test (2004) called Duration Test.\n",
    "        The main objective is to know if the VaR model responds quickly to market movements\n",
    "         in order to do not form volatility clusters.\n",
    "        Duration is time betwenn violations of VaR.\n",
    "        This test verifies if violations has no memory i.e. should be independent.\n",
    "        \n",
    "        Parameters:\n",
    "            violations (series): series of violations of VaR\n",
    "            conf_level (float):  test confidence level\n",
    "        Returns:\n",
    "            answer (dict):       statistics and decision of the test\n",
    "    '''\n",
    "    if isinstance(violations, pd.core.series.Series):\n",
    "        N = violations[violations==0].count()\n",
    "        first_hit = violations.iloc[0]\n",
    "        last_hit = violations.iloc[-1]\n",
    "    elif isinstance(violations, pd.core.frame.DataFrame):\n",
    "        N = violations[violations==0].count().values[0]\n",
    "        first_hit = violations.iloc[0][0]\n",
    "        last_hit = violations.iloc[-1][0]\n",
    "        \n",
    "    duration = [i for i, x in enumerate(violations) if x==1]\n",
    "    \n",
    "    diff_duration = np.diff(duration)\n",
    "    \n",
    "    TN = len(violations)\n",
    "    C = np.zeros(len(diff_duration))\n",
    "    \n",
    "    if not duration:\n",
    "        D=np.array([0,0])\n",
    "        C=np.array([0,0])\n",
    "    \n",
    "    if first_hit==0 and duration:\n",
    "        C = np.append(1,C)\n",
    "        D = np.append(duration[0], diff_duration) #days until first violation\n",
    "        \n",
    "    if last_hit==0 and duration:\n",
    "        C = np.append(C, 1)\n",
    "        D = np.append(D, TN-duration[-1]-1)\n",
    "        \n",
    "    if N>0 and duration: N = len(D)-1\n",
    "    else: N=0\n",
    "      \n",
    "    def likDurationW(x, D, C, N):\n",
    "        b = x\n",
    "        a = ( (N - C[0] - C[N])/(sum(D**b)) )**(1/b)\n",
    "        lik = C[0]* np.log(pweibull(D[0],a,b,survival=True)) + (1-C[0]) * dweibull(D[0], a, b, log = True) +\\\n",
    "            sum(dweibull(D[1:(N-1)], a, b, log = True) ) + C[N]*np.log(pweibull(D[N],a,b,survival = True) )  +\\\n",
    "                (1 - C[N]) *dweibull(D[N], a, b, log = True)\n",
    "                \n",
    "        if np.isnan(lik) or np.isinf(lik): \n",
    "            lik = 1e10\n",
    "        else: lik = -lik\n",
    "        return lik  \n",
    "    \n",
    "    # When b=1 we get the exponential\n",
    "    def dweibull(D, a, b, log=False):\n",
    "        # density of Weibull\n",
    "        pdf = b * np.log(a) + np.log(b) + (b - 1) * np.log(D) - (a * D)**b\n",
    "        if not log: pdf = np.exp(pdf)\n",
    "        return pdf\n",
    "    \n",
    "    def pweibull(D, a, b, survival = False):\n",
    "        # distribution of Weibull\n",
    "        cdf = 1 - np.exp(-(a*D)**b)\n",
    "        if survival: cdf = 1 - cdf\n",
    "        return cdf\n",
    "    \n",
    "    optimizedBetas = optimize.minimize(likDurationW, x0=[2], args=(D, C, N), method=\"L-BFGS-B\",\n",
    "                                       bounds= [(0.001, 10)] )\n",
    "    \n",
    "    print(optimizedBetas.message)\n",
    "    \n",
    "    b = optimizedBetas.x\n",
    "    uLL = -likDurationW(b, D, C, N)\n",
    "    rLL = -likDurationW(1, D, C, N)\n",
    "    LR = 2*(uLL - rLL)\n",
    "    LRp = 1 - chi2.cdf(LR, 1)\n",
    "    \n",
    "    H0 = \"Duration Between Exceedances have no memory (Weibull b=1 = Exponential)\"\n",
    "    #i.e. whether we fail to reject the alternative in the LR test that b=1 (hence correct model)\n",
    "    if LRp<(1-conf_level): \n",
    "        decision = \"Reject H0\"\n",
    "    else: decision = \"Fail to Reject H0\"\n",
    "    \n",
    "    answer = {\"weibull exponential\":b,\n",
    "              \"unrestricted log-likelihood\":uLL,\n",
    "              \"restricted log-likelihood\":rLL,\n",
    "              \"log-likelihood\":LR,\n",
    "              \"log-likelihood ratio test statistic\":LRp,\n",
    "              \"null hypothesis\":H0,\n",
    "              \"decision\":decision}\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def failure_rate(violations):\n",
    "    TN = len(violations)\n",
    "    N = violations.sum()\n",
    "    print(f\"Failure rate of {round((N/TN)*100,2)}%\")\n",
    "    return N/TN\n",
    "\n",
    "def kupiec_test(violations, var_conf_level=0.99, conf_level=0.95):\n",
    "   \n",
    "    '''Perform Kupiec Test (1995).\n",
    "       The main goal is to verify if the number of violations, i.e. proportion of failures, is consistent with the\n",
    "       violations predicted by the model.\n",
    "       \n",
    "        Parameters:\n",
    "            violations (series):    series of violations of VaR\n",
    "            var_conf_level (float): VaR confidence level\n",
    "            conf_level (float):     test confidence level\n",
    "        Returns:\n",
    "            answer (dict):          statistics and decision of the test\n",
    "    '''\n",
    "    if isinstance(violations, pd.core.series.Series):\n",
    "        v = violations[violations==1].count()\n",
    "    elif isinstance(violations, pd.core.frame.DataFrame):\n",
    "        v = violations[violations==1].count().values[0]\n",
    "\n",
    "    N = violations.shape[0]\n",
    "    theta= 1-(v/N)\n",
    "\n",
    "    if v < 0.001:\n",
    "        V = -2*np.log((1-(v/N))**(N))\n",
    "    else:\n",
    "        part1 = ((1-var_conf_level)**(v)) * (var_conf_level**(N-v))\n",
    "        \n",
    "        part11= ((1-theta)**(v)) * (theta**(N-v))\n",
    "        \n",
    "        fact = math.factorial(N) / ( math.factorial(v) * math.factorial(N-v))\n",
    "        \n",
    "        num1 = part1 * fact\n",
    "        den1 = part11 * fact \n",
    "    \n",
    "        V = -2*(np.log(num1/den1))\n",
    "    \n",
    "    chi_square_test = chi2.cdf(V,1) #one degree of freedom\n",
    "    \n",
    "    if chi_square_test < conf_level: result = \"Fail to reject H0\"\n",
    "    elif v==0 and N<=255 and var_conf_level==0.99: result = \"Fail to reject H0\"\n",
    "    else: result = \"Reject H0\"\n",
    "        \n",
    "    return {\"statictic test\":V, \"chi square value\":chi_square_test, \n",
    "            \"null hypothesis\": f\"Probability of failure is {round(1-var_conf_level,3)}\",\n",
    "            \"result\":result}\n",
    "\n",
    "def berkowtiz_tail_test(pnl, volatility_window=252, \n",
    "                        var_conf_level=0.99, conf_level=0.95, random_seed=443):\n",
    "    '''Perform Berkowitz Test (2001).\n",
    "        The goal is to verify if conditional distributions of returns \"GARCH(1,1)\" \n",
    "        used in the VaR Model is adherent to the data.\n",
    "        In this specific test, we do not observe the whole data, only the tail.\n",
    "        \n",
    "        Parameters:\n",
    "            data (dataframe):        pnl (distribution of profit and loss) or return\n",
    "            volatility_window (int): window to cabibrate volatility GARCH model\n",
    "            var_conf_level (float):  VaR confidence level\n",
    "            conf_level (float):      test confidence level\n",
    "            random_seed (int):       integer value to set seed to random values of the optimizer\n",
    "        Returns:\n",
    "            answer (dict):           statistics and decision of the test\n",
    "    '''\n",
    "        \n",
    "    print(\"Normalizing returns...\")\n",
    "    conditional_vol, conditional_mean = pd.DataFrame(), pd.DataFrame()\n",
    "    for t in tqdm(range(pnl.shape[0]-volatility_window+1)):\n",
    "        am = arch.arch_model(pnl[(t):(volatility_window+t)], vol='garch', dist=\"Normal\", rescale=False).fit(disp=\"off\")\n",
    "        cond_vol = am.forecast(horizon=1, reindex=False).variance.apply(np.sqrt)\n",
    "        cond_mean = am.forecast(horizon=1, reindex=False).mean\n",
    "        conditional_vol = pd.concat([conditional_vol, cond_vol])\n",
    "        conditional_mean = pd.concat([conditional_mean, cond_mean])\n",
    "    \n",
    "    ret_padr = ((pnl.values - conditional_mean.values) / conditional_vol.values)\n",
    "        \n",
    "    zeta = stats.norm.ppf(stats.norm.cdf(ret_padr))\n",
    "\n",
    "    alpha=1-var_conf_level\n",
    "    significance = 1-conf_level\n",
    "    \n",
    "    def objective(x):\n",
    "        #pars[0] => media\n",
    "        #pars[1] => vol incondicional\n",
    "        p1 = zeta[np.where(zeta<stats.norm.ppf((alpha)))]\n",
    "        p2 = zeta[np.where(zeta>=stats.norm.ppf(alpha))]*0 + stats.norm.ppf(alpha)\n",
    "        return -( sum(np.log(stats.norm.pdf((p1-x[0])/x[1])/x[1])) + sum( np.log(1-stats.norm.cdf((p2 - x[0])/x[1]) )) )\n",
    "        \n",
    "    print(\"Optimizing...\")\n",
    "    start = time.time()\n",
    "    optimum_result = pygosolnp.solve(\n",
    "                            obj_func=objective,\n",
    "                            par_lower_limit=[-10, 0.01],\n",
    "                            par_upper_limit=[10, 3],\n",
    "                            number_of_simulations=200,  # This represents the number of starting guesses to use\n",
    "                            number_of_restarts=20,  # This specifies how many restarts to run from the best starting guesses\n",
    "                            number_of_processes=None,  # None here means to run everything single-processed\n",
    "                            seed=random_seed,  # Seed for reproducibility, if omitted the default random seed is used (typically cpu clock based)\n",
    "                            pysolnp_max_major_iter=100,  # Pysolnp property\n",
    "                            debug=False)\n",
    "    print(\"\")\n",
    "    print(f\"Elapsed time: {time.time() - start} s\")\n",
    "    \n",
    "    #all_results = optimum_result.all_results\n",
    "    #print(\"; \".join([f\"Solution {index + 1}: {solution.obj_value}\" for index, solution in enumerate(all_results)]))\n",
    "    best_solution = optimum_result.best_solution\n",
    "    #print(f\"Best solution {best_solution.obj_value} for parameters {best_solution.parameters}.\")\n",
    "    \n",
    "    uLL = -best_solution.obj_value\n",
    "    rLL = -objective([0, 1])\n",
    "    LR = 2 * (uLL - rLL)\n",
    "    chid = 1 - stats.chi2.cdf(LR, 2)\n",
    "    if (chid < significance):\n",
    "        decision = \"Reject H0\"\n",
    "    else: decision = \"Fail to Reject H0\"\n",
    "    H0 = \"Distribuition is Normal(0,1)\"\n",
    "    \n",
    "    answer = {\"solution\":best_solution,\n",
    "              \"ull\":uLL,\n",
    "              \"rll\":rLL,\n",
    "              \"LR\":LR,\n",
    "              \"chi square test\":chid,\n",
    "              \"null hypothesis\":H0,\n",
    "              \"decision\":decision}\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ee1e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function zero_mean_test at 0x000001C5BFD4CE50>\n"
     ]
    }
   ],
   "source": [
    "print(zero_mean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0dec25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
